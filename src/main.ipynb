{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac29148",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "Theoretical explanation is present in this blogpost [link](). The project aims to use Bidirectional-LSTM with Conditional Random Fields along with static word embeddings to perform Named Entity Recognition task based off of the CoNLL-2003 dataset. This will be done step by step from downloading and preparing the dataset, embeddings, to training and validating the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca92aec",
   "metadata": {},
   "source": [
    "## 1. Dataset downloading and preprocessing\n",
    "The first step is to download both the dataset which contains the tagged information as well as the static embeddings GloVe, which has vectors with embedded meanings for words. Since BiLSTM cannot generated contextualised vectors like BERT and other transformer models do (a natural improvement), we fallback to using GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f3a2b",
   "metadata": {},
   "source": [
    "### 1.1 Downloading GloVe Embeddings\n",
    "We will download the 100 dimensional variant of GloVe (there's a 300 dimensional variant which is heavy but more accurate). This is available for your exploration at [Stanford NLP Github](https://github.com/stanfordnlp/GloVe/releases). The following script uses utility function to download this automatically and unzips to `src/data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b542e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5a4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.download import download_glove_embeddings, download_conll2003_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8691d5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings already exist. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_glove_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e523a0c0",
   "metadata": {},
   "source": [
    "### 1.2 Downloading the CoNLL2003 Dataset\n",
    "The following method downloads the CoNLL2003 NER dataset in the same location as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8db71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoNLL-2003 dataset already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_conll2003_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81693e",
   "metadata": {},
   "source": [
    "### 1.3 Loading the GloVe embeddings, CoNLL Data and create the dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7e9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.data import load_glove_embeddings, load_conll_dataset\n",
    "from util.dataset import create_data_loader\n",
    "from util.adapters import bio_tag_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61133e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings from file.\n",
      "Loaded 400002 words into vocabulary from GloVe.\n",
      "Loading CoNLL data from data/conll2003/train.txt\n",
      "Dataset loaded.\n",
      "Loading CoNLL data from data/conll2003/test.txt\n",
      "Dataset loaded.\n",
      "Loading CoNLL data from data/conll2003/valid.txt\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the GloVe embeddings\n",
    "word2idx, embeddings, dimensions = load_glove_embeddings()\n",
    "\n",
    "# Load the CoNLL as data\n",
    "train_data, test_data, val_data = load_conll_dataset()\n",
    "# Use the Train data as sample to get the tag dictionary\n",
    "tag2label, label2tag = bio_tag_dictionary(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec20bf",
   "metadata": {},
   "source": [
    "### 1.4 Sanity Checks \n",
    "To ensure data has loaded in the format we expect, to prevent unnecessary bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f2f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dimensions == 100\n",
    "assert word2idx[\"the\"] == 2\n",
    "np.testing.assert_allclose(\n",
    "    embeddings[word2idx[\"the\"]],\n",
    "    np.array([-0.038194, -0.24487, 0.72812, -0.39961, 0.083172, 0.043953, -0.39141, 0.3344, -0.57545, 0.087459, 0.28787, -0.06731, 0.30906, -0.26384, -0.13231, -0.20757, 0.33395, -0.33848, -0.31743, -0.48336, 0.1464, -0.37304, 0.34577, 0.052041, 0.44946, -0.46971, 0.02628, -0.54155, -0.15518, -0.14107, -0.039722, 0.28277, 0.14393, 0.23464, -0.31021, 0.086173, 0.20397, 0.52624, 0.17164, -0.082378, -0.71787, -0.41531, 0.20335, -0.12763, 0.41367, 0.55187, 0.57908, -0.33477,\n",
    "              -0.36559, -0.54857, -0.062892, 0.26584, 0.30205, 0.99775, -0.80481, -3.0243, 0.01254, -0.36942, 2.2167, 0.72201, -0.24978, 0.92136, 0.034514, 0.46745, 1.1079, -0.19358, -0.074575, 0.23353, -0.052062, -0.22044, 0.057162, -0.15806, -0.30798, -0.41625, 0.37972, 0.15006, -0.53212, -0.2055, -1.2526, 0.071624, 0.70565, 0.49744, -0.42063, 0.26148, -1.538, -0.30223, -0.073438, -0.28312, 0.37104, -0.25217, 0.016215, -0.017099, -0.38984, 0.87424, -0.72569, -0.51058, -0.52028, -0.1459, 0.8278, 0.27062]),\n",
    "    rtol=1e-5,\n",
    "    atol=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4efa6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
       " ['b-org', 'o', 'b-misc', 'o', 'o', 'o', 'b-misc', 'o', 'o'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "\n",
    "# Create the actual dataloader\n",
    "train_data_loader = create_data_loader(\n",
    "    train_data, word2idx, tag2label, embeddings, batch_size=BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(\n",
    "    val_data, word2idx, tag2label, embeddings, batch_size=BATCH_SIZE, \n",
    "    is_train=False)\n",
    "test_data_loader = create_data_loader(\n",
    "    test_data, word2idx, tag2label, embeddings, batch_size=BATCH_SIZE, \n",
    "    is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4bf032",
   "metadata": {},
   "source": [
    "## 2. Creating the Runner Instance\n",
    "To run the training and validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from runner import Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f402ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants here\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(LEARNING_RATE, train_data_loader, val_data_loader,\n",
    "                test_data_loader, dimensions, len(tag2label.keys()), 30, \"\", label2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f76c60",
   "metadata": {},
   "source": [
    "### 2.1. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436ebd8",
   "metadata": {},
   "source": [
    "### 2.2. Testing with test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ENTER PATH TO FILE HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a63745",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(LEARNING_RATE, train_data_loader, val_data_loader,\n",
    "                test_data_loader, dimensions, len(tag2label.keys()), 30, filename, label2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e48f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
